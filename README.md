# Zillow Project

## Purpose
This project aims to improve our original estimate of the log error by using clustering methodologies.

The customer is the Curie data science team.

## Deliverables
1. Github Repository: [Link](https://github.com/clustering-zillow)
    - Readme (this file)
    - Final Jupyter notebook walking through the pipeline
    - .py files with all functions necessary to reproduce the model

## Pipeline

# Planning
- Hypotheses:
    - $H_0$: Engineered features will not reduce logerror
    - $H_a$: Engineered features produced by clustering will reduce logerror

- Predictor Features (independent):
    - 

- Predicted Feature (dependent): 'logerror' 

# Acquire Data
- acquire.py has all necessary functions to generate this dataframe from SQL

# Data Preparation
- wrangle_zillow has all functions listed here
    - address missing & null values, data integrity issues, etc.
        - 
    - plot distributions of variables
        - identify outliers
        - decide how/if to scale features
        - find errors or invalid data
    - data dictionary
        1. parcelid = identifier of each property
        2. baths = number of bathrooms. Used field named 'bathroomcnt' instead of 'calculatedbathnbr' or 'fullbathcnt' because the other two fields included nulls, and were essentially the same. Dropped rows with 0 bathrooms, to ensure I was only looking at single residence homes.
        3. beds = number of bedrooms. No other fields seemed to contain this info. Dropped rows with 0 bedrooms, to ensure I was only looking at single residence homes.
        4. home_sf = size of home. Used field 'calculatedfinishedsquarefeet' because it was identical to 'finishedsquarefeet12', the other square feet fields were mostly nulls.
        5. lot_sf = size of lot. Used field lotsizesquarefeet.
        6. roomcnt = number of rooms other than beds or baths. 
        7. value = value of property. Used field named 'taxvaluedollarcnt' because it was specified in the project specifications.
        8. has_garage = boolean field generated from garagecarcnt, where null values are 0, and anything else is a 1.
        9. has_base = boolean field generated from basementsqft, where null values are 0, and anything else is a 1.
        10. has_AC = boolean field generated from airconditioningtypeid, where null values are 0, and anything else is a 1.
        11. has_fire = boolean field generated from fireplacecnt, where null values are 0, and anything else is a 1.
        12. has_pool = boolean field generated from poolcnt, where null values are 0, and anything else is a 1.
        13. has_deck = boolean field generated from decktypeid, where null values are 0, and anything else is a 1.
        14. has_hottub = boolean field generated from hashottuborspa, where null values are 0, and anything else is a 1.
        15. is_extra = calculated field generated by summing together fields 8 - 14, the bigger the value, the more extra features the home has.
        16. all_rooms = calculated field generated by summing together baths, beds & roomcnt.
        17. factor = calculated field generated to proxy the avg sq footage of homes in each neghborhood.
        18. sq_feet_proxy6 = calulated field generated by dividing the sq footage of the property by the factor field.
        19. county = name of county the property is from. This field came from the fips identifier in the original databse. The website I obtained the names of counties from: [link] (https://www.nrcs.usda.gov/wps/portal/nrcs/detail/national/home/?cid=nrcs143_013697)
        20. state = name of state. This field came from the fips identifier in the original databse. The website I obtained the names of counties from: [link] (https://www.nrcs.usda.gov/wps/portal/nrcs/detail/national/home/?cid=nrcs143_013697)
        21. tax_rate = calculated field created by dividing the value of the property by the tax_amount.
        22. size_ratio = calculated field generated by dividing home_sf by lot_sf
        23. age = calculated field generated by subtracting 2017 from yearbuilt

# Split & Scale
- Goal: two prepared dataframes (train & test) and scaled data
    - Create a file called split_scale.py with all necessary functions to generate this dataframe

# Data Exploration
- Goal: Address questions generated during planning and preparation phases
    - Run at least one t-test and one correlation test
    - Visualize all combinations of variables
    - 
    - Summarize takeaways and conclusions

# Feature Selection
- Goal: a dataframe with the features to be used to build the model
    - Are there new features that can be generated from existing features? (only to be done if there is time)
    - Use feature selection techniques
    - Create a file called feature_selection.py with all necessary functions to generate this dataframe

# Modeling & Evaluation
- Goal: develop a regression model that performs better than a baseline
    - Create baseline model
    - Show how model outperforms baseline
    - Notebook has extra algorithms and hyperparameters tried, with evaluation code and results
    - Evaluate by:
        - plot residuals
        - compute evaluation metrics (SSE, MSE, RMSE)
        - compare to baseline
        - plot y(actual) by y(predicted)
    - Create a file called model.py with all necessary functions to fit, predict and evaluate the model